{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "established-trader",
   "metadata": {},
   "source": [
    "# TFIDF - NMF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5b06b4-a78a-49ca-9875-f7f732ba7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "with open('../data/euroleaks/stopwords.json', 'r') as f:\n",
    "    stopwords = json.load(f)\n",
    "    \n",
    "# collocations\n",
    "def apply_trigram_colloc(s, set_colloc):\n",
    "    res = s.lower()\n",
    "    for b1,b2,b3 in set_colloc:\n",
    "        res = res.replace(f'{b1} {b2} {b3}', f'{b1}_{b2}_{b3}')\n",
    "    return res\n",
    "\n",
    "def apply_bigram_colloc(s, set_colloc):\n",
    "    res = s.lower()\n",
    "    for b1,b2 in set_colloc:\n",
    "        res = res.replace(f'{b1} {b2}', f'{b1}_{b2}')\n",
    "    return res\n",
    "\n",
    "with open('../data/collocations/trigrams.json', 'r') as f:\n",
    "    trigram_colloc = json.load(f)\n",
    "\n",
    "with open('../data/collocations/bigrams.json', 'r') as f:\n",
    "    bigram_colloc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cebc7e-7ed4-4339-abcc-f8a72874a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449ee14-919e-46a2-ba57-1bfba09fe629",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0203fa4-3b88-4bca-a857-c69df3cd5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_token(token):\n",
    "    return token.pos_ in {'ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB'}\\\n",
    "        and not token.lemma_.lower() in nlp.Defaults.stop_words\\\n",
    "        and not token.lower_ in stopwords['names']\\\n",
    "        and not token.lower_ in stopwords['disfluency']\\\n",
    "        and not token.lemma_.lower() in stopwords['courtesy']\\\n",
    "        and len(token.lemma_) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-operation",
   "metadata": {},
   "source": [
    "# Euroleaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-shape",
   "metadata": {},
   "source": [
    "## document = speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-broad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaks = pd.read_csv('../data/euroleaks/squeezed.csv')\n",
    "\n",
    "# preprocess\n",
    "leaks_documents = [\n",
    "    ' '.join([token.lemma_.lower() for sentence in nlp(doc).sents for token in sentence\n",
    "              if filter_token(token)\n",
    "             ])\n",
    "             for doc in leaks.speech.values\n",
    "]\n",
    "\n",
    "# leave out empty documents\n",
    "leaks_documents = [d for d in leaks_documents if len(d)>1]\n",
    "\n",
    "# apply collocations\n",
    "leaks_documents = [\n",
    "    apply_bigram_colloc(apply_trigram_colloc(doc, trigram_colloc), bigram_colloc)\n",
    "    for doc in leaks_documents]\n",
    "\n",
    "# tfidf\n",
    "leaks_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   min_df=10,\n",
    "                                   max_df=0.75,\n",
    "                                   smooth_idf=True,\n",
    "                                   sublinear_tf=False)\n",
    "\n",
    "leaks_X = leaks_vectorizer.fit_transform(leaks_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74154783-045b-4a45-ab53-31552f3973fd",
   "metadata": {},
   "source": [
    "- **TODO**: what are good min_df and max_df values? task-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e34a74-a38e-4782-91a8-d0d846e66bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'matrix shape: {leaks_X.shape}')\n",
    "\n",
    "leaks_tfidf = leaks_X.sum(axis =0).A1\n",
    "leaks_idf = leaks_vectorizer.idf_\n",
    "\n",
    "plt.scatter(range(len(leaks_tfidf))[5:], np.sort(leaks_tfidf)[::-1][5:], marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.scatter(leaks_tfidf, leaks_idf, s=leaks_tfidf, edgecolors='black', label='tfidf')\n",
    "#ax.legend()\n",
    "ax.set_xlabel('tfidf')\n",
    "ax.set_ylabel('idf')\n",
    "ax.set_title('scatterplot of TFIDF and IDF values, scaled by TF')\n",
    "\n",
    "# annotate words with highest tfidf\n",
    "first_k = 10\n",
    "sort_ix = np.argsort(leaks_tfidf)[::-1]\n",
    "for ix in sort_ix[:first_k]:\n",
    "    s = leaks_vectorizer.get_feature_names()[ix]\n",
    "    jiggle_x = np.random.normal(0,0.1)\n",
    "    jiggle_y = np.random.normal(0,0.1)\n",
    "    ax.text(leaks_tfidf[ix]+jiggle_x, leaks_idf[ix]+jiggle_y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in np.array(leaks_vectorizer.get_feature_names())[sort_ix][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd16e0-55e6-4b8c-aa0e-137675a34d13",
   "metadata": {},
   "source": [
    "### NMF topic model\n",
    "- https://shravan-kuchkula.github.io/nlp/topic-modeling/#build-nmf-model-using-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c7c51-572e-458b-ad58-1d0e490cc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics with their terms and weights\n",
    "def my_get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "\n",
    "    topics = [{term: float(weight) for term, weight in zip(terms, term_weights)} for terms, term_weights in zip(sorted_terms, sorted_weights)]\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# get topics with their terms and weights\n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "# prints components of all the topics\n",
    "# obtained from topic modeling\n",
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        #print(topic)\n",
    "        topic = [(word, round(wt,2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index)+' with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #'+str(index)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms]) if num_terms else tw\n",
    "        print()\n",
    "\n",
    "# prints components of all the topics\n",
    "# obtained from topic modeling\n",
    "def get_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     num_terms=None):\n",
    "\n",
    "    topic_terms = []\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        #print(topic)\n",
    "        topic = [(word, round(wt,2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "\n",
    "        topic_terms.append(topic[:num_terms] if num_terms else topic)\n",
    "\n",
    "    return topic_terms\n",
    "\n",
    "def getTermsAndSizes(topic_display_list_item):\n",
    "    terms = []\n",
    "    sizes = []\n",
    "    for term, size in topic_display_list_item:\n",
    "        terms.append(term)\n",
    "        sizes.append(size)\n",
    "    return terms, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a4f3-13d9-4899-9fde-e0db9afba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=3, init='nndsvd', alpha=0.1, l1_ratio=0.5)\n",
    "nmf_output = nmf.fit_transform(leaks_X)\n",
    "\n",
    "nmf_feature_names = leaks_vectorizer.get_feature_names()\n",
    "nmf_weights = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba1fae-85da-4a1c-be2a-effda26c7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6ebd0-2318-4b2f-9d15-4fd16b4caf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_terms_weights(nmf_weights, nmf_feature_names)\n",
    "\n",
    "print_topics_udf(topics, total_topics=3, num_terms=20, display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccb5e2-a98c-4d4b-8823-4ee37aa10b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38144a6c-97df-4d95-8133-19cbd6a5928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks.speech[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-badge",
   "metadata": {},
   "source": [
    "## document = speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f7eb4-c362-4f65-8895-d85f4bc549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = pd.read_csv('../data/euroleaks/squeezed.csv')\n",
    "\n",
    "# group by speaker\n",
    "grouped = leaks.drop(columns=['date']).groupby('speaker').apply(lambda s: ' '.join(s.speech))\n",
    "# get speaker labels\n",
    "speakers = grouped.index\n",
    "\n",
    "# make a list of all unidentified speakers\n",
    "unidentified_speakers = [s for s in speakers if 'speaker' in s]\n",
    "unidentified_speakers += [\n",
    "    'irina',\n",
    "    'kian',\n",
    "    'male',\n",
    "    'martin',\n",
    "    'nabil',\n",
    "    #'tooma', # I just know that he represents Finland\n",
    "    'tropa'\n",
    "]\n",
    "\n",
    "# get identified speaker labels\n",
    "identified_speakers = speakers[speakers.to_series().apply(lambda s: s not in unidentified_speakers)]\n",
    "# filter out unidentified speakers\n",
    "grouped = grouped[speakers.to_series().apply(lambda s: s not in unidentified_speakers)]\n",
    "\n",
    "# preprocess\n",
    "leaks_documents = [\n",
    "    ' '.join([token.lemma_.lower() for sentence in nlp(doc).sents for token in sentence\n",
    "              if filter_token(token)\n",
    "             ])\n",
    "             for doc in grouped.values\n",
    "]\n",
    "\n",
    "# leave out empty documents\n",
    "leaks_documents = [d for d in leaks_documents if len(d)>1]\n",
    "\n",
    "print(f'There are {len(leaks_documents)} documents.')\n",
    "\n",
    "# apply collocations\n",
    "leaks_documents = [\n",
    "    apply_bigram_colloc(apply_trigram_colloc(doc, trigram_colloc), bigram_colloc)\n",
    "    for doc in leaks_documents]\n",
    "\n",
    "# need to filter out unidentified speakers first?\n",
    "\n",
    "# tfidf\n",
    "leaks_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   min_df=2,\n",
    "                                   max_df=0.75,\n",
    "                                   smooth_idf=True,\n",
    "                                   sublinear_tf=False)\n",
    "\n",
    "leaks_X = leaks_vectorizer.fit_transform(leaks_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa47b4-7e51-4248-a77e-a19226ea461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(31 * 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58016d66-59c0-4393-8320-bab4fdd2136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers that have less than cutoff tokens after preprocessing\n",
    "cutoff = 100\n",
    "speakers[speakers.to_series().apply(lambda s: s not in unidentified_speakers)][np.array([len(d.split()) for d in leaks_documents]) < cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287962e-236f-4726-93db-8ee37e09326c",
   "metadata": {},
   "source": [
    "What are good min_df and max_df values? Need to choose them specifically for the task at hand; meaning clustering speakers.\n",
    "\n",
    "Therefore, I want to remove words which have low discriminatory power between speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'matrix shape: {leaks_X.shape}')\n",
    "\n",
    "leaks_tfidf = leaks_X.sum(axis =0).A1\n",
    "leaks_idf = leaks_vectorizer.idf_\n",
    "\n",
    "plt.scatter(range(len(leaks_tfidf))[5:], np.sort(leaks_tfidf)[::-1][5:], marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9e482-7fb0-4d03-9305-2fbf23d5a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_tfidf = leaks_X.sum(axis =0).A1\n",
    "sort_ix = np.argsort(leaks_tfidf)[::-1]\n",
    "\n",
    "for word in np.array(leaks_vectorizer.get_feature_names())[sort_ix][:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-classification",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015b56f-74e2-45dd-8253-c73d9e9b10fe",
   "metadata": {},
   "source": [
    "#### cross validation to find number of latent dimenstions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2fb67-dd09-4c06-ab5b-fff34294c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from:\n",
    "# https://gist.github.com/ahwillia/65d8f87fcd4bded3676d67b55c1a3954\n",
    "# nonnegfac: https://github.com/kimjingu/nonnegfac-python\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randn, rand\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from nonnegfac.nnls import nnlsm_blockpivot as nnlstsq\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def censored_nnlstsq(A, B, M):\n",
    "    \"\"\"Solves nonnegative least-squares problem with missing data in B\n",
    "    Args\n",
    "    ----\n",
    "    A (ndarray) : m x r matrix\n",
    "    B (ndarray) : m x n matrix\n",
    "    M (ndarray) : m x n binary matrix (zeros indicate missing values)\n",
    "    Returns\n",
    "    -------\n",
    "    X (ndarray) : nonnegative r x n matrix that minimizes norm(M*(AX - B))\n",
    "    \"\"\"\n",
    "    \n",
    "    if A.ndim == 1:\n",
    "        A = A[:,None]\n",
    "    rhs = np.dot(A.T, M * B).T[:,:,None] # n x r x 1 tensor # dimension mismatch here\n",
    "    T = np.matmul(A.T[None,:,:], M.T[:,:,None] * A[None,:,:]) # n x r x r tensor\n",
    "    X = np.empty((B.shape[1], A.shape[1]))\n",
    "    for n in range(B.shape[1]):\n",
    "        X[n] = nnlstsq(T[n], rhs[n], is_input_prod=True)[0].T\n",
    "    return X.T\n",
    "\n",
    "\n",
    "def cv_nmf(data, rank, M=None, p_holdout=0.3):\n",
    "    \"\"\"Fit NMF while holding out a fraction of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    solver = censored_nnlstsq\n",
    "\n",
    "    # create masking matrix\n",
    "    if M is None:\n",
    "        M = np.random.rand(*data.shape) > p_holdout\n",
    "\n",
    "    # initialize U randomly\n",
    "    U = np.random.rand(data.shape[0], rank)\n",
    "\n",
    "    # fit nmf\n",
    "    for itr in range(50):\n",
    "        Vt = solver(U, data, M)\n",
    "        U = solver(Vt.T, data.T, M.T).T\n",
    "    \n",
    "    # return result and test/train error\n",
    "    resid = np.dot(U, Vt) - data\n",
    "    train_err = np.mean(resid[M]**2)\n",
    "    test_err = np.mean(resid[~M]**2)\n",
    "    return U, Vt, train_err, test_err\n",
    "\n",
    "\n",
    "def validate_nmf(data, M=None):\n",
    "    \n",
    "    # parameters\n",
    "    replicates = 1\n",
    "    ranks = np.arange(1, 4)\n",
    "    \n",
    "    train_err, test_err = [], []\n",
    "\n",
    "    # fit models\n",
    "    for rnk, _ in itertools.product(ranks, range(replicates)):\n",
    "        print(rnk)\n",
    "        tr, te = cv_nmf(data, rnk, M)[2:]\n",
    "        train_err.append((rnk, tr))\n",
    "        test_err.append((rnk, te))\n",
    "        \n",
    "    return train_err, test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae45e5-8bce-4a13-a287-a01626914b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_mask(shape):\n",
    "    return np.vstack([np.eye(shape[1]) for _ in range(int(shape[0] / shape[1])+1)])[:shape[0],:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c8ad9-1d0b-4288-9485-fb0a8fd4d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control with 6 latent dimensions\n",
    "# control_n_latent_dim = 6\n",
    "# M, N = 100, 150\n",
    "# noise = .8\n",
    "\n",
    "# U = np.random.rand(M, control_n_latent_dim)\n",
    "# Vt = np.random.rand(control_n_latent_dim, N)\n",
    "# control_X = np.dot(U, Vt) + noise*np.random.rand(M, N)\n",
    "\n",
    "# control_train_err, control_test_err = validate_nmf(control_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac640ea-4b3c-44ce-985f-69c5fd6147c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to construct a mask that takes sparseness into account, but often singular, no time to go into this\n",
    "\n",
    "# p_holdout = 0.3\n",
    "\n",
    "# nonzero = np.where(leaks_X.A > 0)\n",
    "\n",
    "# mask_ = np.random.rand(nonzero[0].size) > p_holdout\n",
    "\n",
    "# nonzero = (nonzero[0][mask_], nonzero[1][mask_])\n",
    "\n",
    "# mask = np.ones(leaks_X.shape).astype(int)\n",
    "# mask[nonzero] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f1c17-37ef-4650-a4e0-81f7e103a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate tfidf matrix\n",
    "#leaks_train_err, leaks_test_err = validate_nmf(leaks_X.A, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625e917-e16f-4eaa-ae4d-0139ee7db1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# titles = [f'Control ({control_n_latent_dim} latent dimensions)', 'Euroleaks identified speakers']\n",
    "# train_errs = [control_train_err, leaks_train_err]\n",
    "# test_errs = [control_test_err, leaks_test_err]\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "    \n",
    "#     ax.plot(*list(zip(*train_errs[i])), '-xb', label='train')\n",
    "#     ax.plot(*list(zip(*test_errs[i])), '-or', label='test')\n",
    "#     ax.set_title(titles[i])\n",
    "#     #ax.legend()\n",
    "        \n",
    "#     ax.set_xlabel('Number of factors')\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         ax.set_ylabel('Mean Squared Error')\n",
    "#         ax.axvline(control_n_latent_dim, color='k', dashes=[2,2])\n",
    "#     else:\n",
    "#         ax.legend()\n",
    "    \n",
    "# # lines, labels = axes[0].get_legend_handles_labels()\n",
    "# # fig.legend(lines, labels, loc='upper right')\n",
    "# # fig.subplots_adjust(right=0.9)\n",
    "    \n",
    "# fig.tight_layout()\n",
    "# #fig.savefig('../figures/nmf-speaker-validation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb09aee-b69b-45ea-959e-dffa103ebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percent of entries in X which are 0: {np.round(np.mean(leaks_X.A == 0)*100,2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-advocacy",
   "metadata": {},
   "source": [
    "#### 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=3,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          random_state=0, # important sicne otherwise the resulting plot will be different each run\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(leaks_X.toarray())\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bb743-9648-43eb-912a-f45d16085500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get speaker labels (name + entity they represent)\n",
    "with open('../data/euroleaks/name_to_entity.json', 'r') as f:\n",
    "    speaker_to_entity = json.load(f)\n",
    "\n",
    "institutions = ['ECB', 'IMF', 'European Commission', 'ESM', 'EFC']\n",
    "markers = ['\\u2020', '\\u2021']\n",
    "\n",
    "def intervention(s):\n",
    "    if s not in speaker_to_entity.keys():\n",
    "        return 'Unidentified'\n",
    "    if speaker_to_entity[s] == 'European Commission':\n",
    "        return 'EC'\n",
    "    return speaker_to_entity[s]\n",
    "\n",
    "def make_label_from_speaker(s):\n",
    "    return f'{markers[0] if speaker_to_entity[s] in institutions else markers[1]} {s.title()} ({intervention(s)})'\n",
    "\n",
    "labels = identified_speakers.to_series().apply(lambda s: make_label_from_speaker(s)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148af07d-c7cc-4f21-9d8a-edd5bf833924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct text annotations so that they don't overlap\n",
    "\n",
    "offset = [\n",
    "    (.01,.0,.025), # Alexander Stubb\n",
    "    (.01,.01,.01), # Benoît Cœuré\n",
    "    (.01,-.08,-.01), # Christine Lagarde \n",
    "    (-.04,.07,-.16), # Declan Costello\n",
    "    (.01,.01,.01), # Dušan Mramor\n",
    "    (-.15,-.18,.03), # Edward Scicluna  \n",
    "    (.01,.01,.01), # Hans Jörg Schelling  \n",
    "    (.015,.0,-.02), # Harris Georgiades\n",
    "    (.02,-.005,.01), # Jeroen Dijsselbloem\n",
    "    (-.23,-.18,-.03), # Johan Van Overtveldt\n",
    "    (-.15,-.1,-.03), # Jānis Reirs\n",
    "    (.015,.005,.0), # Klaus Regling\n",
    "    (-.01,-.02,-.06), # Luca Antonio Ricci\n",
    "    (.02,-.05,.0), # Luis De Guindos\n",
    "    (-.13,-.08,-.05), # Luis Pierre\n",
    "    (-.13,-.08,.005), # Marco Buti\n",
    "    (.01,.01,.01), # Maria Luís Albuquerque\n",
    "    (.01,.01,.01), # Mario Draghi\n",
    "    (-.17,-.12,-.1), # Michael Noonan\n",
    "    (-.16,-.12,-.05), # Michel Sapin\n",
    "    (-.23,-.13,-.06), # Nikos Theocarakis\n",
    "    (-.15,-.18,.005), # Peter Kažimír\n",
    "    (.02,-.01,.0), # Pier Carlo Padoan\n",
    "    (.01,.01,.0), # Pierre Moscovici\n",
    "    (.02,.0,.0), # Poul Mathias Thomsen\n",
    "    (.01,.015,.005), # Rimantas Šadžius\n",
    "    (.01,.01,.01), # Thomas Steffen\n",
    "    (.01,.015,-.01), # Thomas Wieser\n",
    "    (-.12,-.12,.0), # Tooma\n",
    "    (.01,.01,.0), # Wolfgang Schäuble\n",
    "    (.01,.01,.0), # Yanis Varoufakis\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# remove axis ticks\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "\n",
    "for counter, (point, label) in enumerate(zip(W, labels)):\n",
    "\n",
    "    x,y,z = point\n",
    "    ax.scatter( x, y, z,\n",
    "                color=(x,y,z),\n",
    "                edgecolor=None,\n",
    "                alpha=0.3,\n",
    "                s=100)\n",
    "    \n",
    "    ax.text(x + offset[counter][0],\n",
    "            y + offset[counter][1],\n",
    "            z + offset[counter][2],\n",
    "            label,\n",
    "            zdir=(1,1,0))\n",
    "\n",
    "#ax.axis('off')\n",
    "\n",
    "#fig.savefig('../figures/3d-speakers-leaks.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eeff69-ce63-4df0-ad2a-9636a66176b2",
   "metadata": {},
   "source": [
    "#### clustering\n",
    "Each axis captures the base topic of a particular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d2540-4c5e-4446-8127-ecc0c3c3b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(identified_speakers)} speakers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70b053-df79-4d4a-82f4-a7d440da18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3\n",
    "\n",
    "nmf = NMF(n_components=number_of_clusters,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(leaks_X.toarray())\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a9a16-414b-486d-9e71-3b8423b8937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for speaker, cluster in  zip(identified_speakers, W.argmax(axis=1)):\n",
    "    clusters[cluster].append(speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da908cf8-e1d6-4eb0-b232-74ed9c4506ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,values in clusters.items():\n",
    "    print(f'cluster {key}:')\n",
    "    for v in values:\n",
    "        print(f'\\t{v}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c6c03-8a25-4eaa-a43a-ee1668859961",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_terms_weights(H, leaks_vectorizer.get_feature_names())\n",
    "\n",
    "print_topics_udf(topics, total_topics=3, num_terms=15, display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd8c4d-80ae-41be-bd67-7ac1c9054b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_from_speaker(s):\n",
    "    return f'{markers[0] if speaker_to_entity[s] in institutions else markers[1]} {s.title()}\\\\newline({speaker_to_entity[s] if s in speaker_to_entity.keys() else \"Unkown\"})'\n",
    "\n",
    "\n",
    "def format_latex_table(first_n, topics, clusters):\n",
    "\n",
    "    header = 'topic 0 & topic 1 & topic 2 \\\\\\\\'\n",
    "\n",
    "    print('\\hline')\n",
    "    print(header)\n",
    "    print('\\hline')\n",
    "    \n",
    "    for j in range(max([len(c) for c in clusters.values()])):\n",
    "        \n",
    "        #print('\\\\centering ', end='')\n",
    "        \n",
    "        for i in range(len(topics)):\n",
    "\n",
    "            if j < len(clusters[i]):\n",
    "                print(f'{make_label_from_speaker(clusters[i][j])} ', end='')\n",
    "\n",
    "            if i < len(topics)-1:\n",
    "                print('& ', end='')\n",
    "                \n",
    "        print('\\\\\\\\')\n",
    "        \n",
    "    print('\\hline')\n",
    "\n",
    "    for j in range(first_n):\n",
    "        \n",
    "        #print('\\\\centering ', end='')\n",
    "        \n",
    "        for i in range(len(topics)):\n",
    "\n",
    "            word = topics[i][j,0].replace(\"_\", \"\\\\_\")\n",
    "            if i == len(topics) - 1:\n",
    "                print(f'{word} \\\\\\\\')\n",
    "            else:\n",
    "                print(f'{word} & ', end='')\n",
    "\n",
    "    print('\\hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ebda2-56ab-486f-a989-c6cc1365bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_latex_table(10, topics, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-quest",
   "metadata": {},
   "source": [
    "## document = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddf2fc-4b75-40b1-afee-28dfa0d046fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = pd.read_csv('../data/euroleaks/squeezed.csv')\n",
    "\n",
    "# group by speaker\n",
    "grouped = leaks.drop(columns=['speaker']).groupby('date').apply(lambda s: ' '.join(s.speech))\n",
    "# get speaker labels\n",
    "dates = grouped.index\n",
    "\n",
    "# preprocess\n",
    "leaks_documents = [\n",
    "    ' '.join([token.lemma_.lower() for sentence in nlp(doc).sents for token in sentence\n",
    "              if filter_token(token)\n",
    "             ])\n",
    "             for doc in grouped.values\n",
    "]\n",
    "\n",
    "# leave out empty documents\n",
    "leaks_documents = [d for d in leaks_documents if len(d)>1]\n",
    "\n",
    "print(f'There are {len(leaks_documents)} documents.')\n",
    "\n",
    "# apply collocations\n",
    "leaks_documents = [\n",
    "    apply_bigram_colloc(apply_trigram_colloc(doc, trigram_colloc), bigram_colloc)\n",
    "    for doc in leaks_documents]\n",
    "\n",
    "# need to filter out unidentified speakers first?\n",
    "\n",
    "# tfidf\n",
    "leaks_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   min_df=2,\n",
    "                                   max_df=0.75,\n",
    "                                   smooth_idf=True,\n",
    "                                   sublinear_tf=False)\n",
    "\n",
    "leaks_X = leaks_vectorizer.fit_transform(leaks_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7f04e-2335-4a68-8840-c7a0cc608bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(12*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421e9a5-dfae-49cd-9ccb-fc69c96a4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052aeaa-f1e4-4228-b9c5-6e476d749ab6",
   "metadata": {},
   "source": [
    "### cross validation to find number of latent dimenstions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ac695-a6e0-4487-abeb-fe067059cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control with 6 latent dimensions\n",
    "# control_n_latent_dim = 3\n",
    "# M, N = 100, 150\n",
    "# noise = .8\n",
    "\n",
    "# U = np.random.rand(M, control_n_latent_dim)\n",
    "# Vt = np.random.rand(control_n_latent_dim, N)\n",
    "# control_X = np.dot(U, Vt) + noise*np.random.rand(M, N)\n",
    "\n",
    "# control_train_err, control_test_err = validate_nmf(control_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d19e6d-f459-42ac-b820-43f0910ed2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate tfidf matrix\n",
    "#leaks_train_err, leaks_test_err = validate_nmf(leaks_X.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1acc4c-46e3-4713-ae75-ac1ce5782f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# titles = [f'Control ({control_n_latent_dim} latent dimensions)', 'Euroleaks identified speakers']\n",
    "# train_errs = [control_train_err, leaks_train_err]\n",
    "# test_errs = [control_test_err, leaks_test_err]\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "    \n",
    "#     ax.plot(*list(zip(*train_errs[i])), '-xb', label='train')\n",
    "#     ax.plot(*list(zip(*test_errs[i])), '-or', label='test')\n",
    "#     ax.set_title(titles[i])\n",
    "#     #ax.legend()\n",
    "        \n",
    "#     ax.set_xlabel('Number of factors')\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         ax.set_ylabel('Mean Squared Error')\n",
    "#         ax.axvline(control_n_latent_dim, color='k', dashes=[2,2])\n",
    "    \n",
    "# lines, labels = axes[0].get_legend_handles_labels()\n",
    "# fig.legend(lines, labels, loc='upper right')\n",
    "# fig.subplots_adjust(right=0.9)\n",
    "    \n",
    "# #fig.tight_layout()\n",
    "# fig.savefig('../figures/nmf-date-validation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b13a12-b241-4737-ae1c-474cd2837b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percent of entries in X which are 0: {np.round(np.mean(leaks_X.A > 0)*100,2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-distance",
   "metadata": {},
   "source": [
    "### 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0891104-1611-492a-bc23-b37dbd4a3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=3,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          random_state=0, # important sicne otherwise the resulting plot will be different each run\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(leaks_X.toarray())\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://stackoverflow.com/questions/22867620/putting-arrowheads-on-vectors-in-matplotlibs-3d-plot\n",
    "\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d618d-8849-4e24-9ab0-6fd4ae3732fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = [\n",
    "    (-.055,-.055,.0),\n",
    "    (-.055,-.055,.0),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (-.055,-.055,-.01),\n",
    "    (-.055,-.055,-.01),\n",
    "    (-.055,-.055,.0),\n",
    "    (.01,.01,.01),\n",
    "    \n",
    "    (-.055,-.085,-.01),\n",
    "    (.04,-.04,.01),\n",
    "    (.01,.01,.01),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# remove axis ticks\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "\n",
    "for i, label in enumerate(dates):\n",
    "\n",
    "    point = W[i,:]\n",
    "    \n",
    "    # plot\n",
    "    x,y,z = point\n",
    "    c = (min(max(x,0),1),min(max(y,0),1),min(max(z,0),1))\n",
    "    ax.scatter( x, y, z,\n",
    "                color=c,\n",
    "                edgecolor=None,\n",
    "                alpha=0.5,\n",
    "                s=100)\n",
    "    \n",
    "    # annotate\n",
    "    ax.text(x + offset[i][0],\n",
    "            y + offset[i][1],\n",
    "            z + offset[i][2],\n",
    "            pd.to_datetime(label).strftime('%d/%m'),\n",
    "            zdir=(1,1,0))\n",
    "    \n",
    "    # add arrow\n",
    "    if i>0:\n",
    "        previous_point = W[i-1,:]\n",
    "        px, py, pz = previous_point\n",
    "        #ax.arrow((px,x),(py,y),(pz,z),\n",
    "        #        color='black',\n",
    "        #        arrowstyle='-|>')\n",
    "        #ax.quiver(px,py,pz,\n",
    "        #          x,y,z,\n",
    "        #          color='black',\n",
    "        #          alpha=0.3,\n",
    "        #          lw=2)\n",
    "        arrow = Arrow3D([px,x],[py,y],[pz,z],\n",
    "                        mutation_scale=20,\n",
    "                        lw=1,\n",
    "                        arrowstyle='-|>',\n",
    "                        color='k',\n",
    "                        alpha=0.3)\n",
    "        ax.add_artist(arrow)\n",
    "\n",
    "fig.savefig('../figures/3d-dates-leaks.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46936e3e-2a53-40c9-ac5b-ddb931c2c8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b074bf-4df7-4080-99b2-f6f47a1afbf9",
   "metadata": {},
   "source": [
    "### clustering\n",
    "Each axis captures the base topic of a particular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4d534-0676-4a1d-ab99-a7c372c0b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03530e76-86df-4d3c-8da6-c9a5cc3a1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3\n",
    "\n",
    "nmf = NMF(n_components=number_of_clusters,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(leaks_X.toarray())\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98762f-093e-4988-b58f-1f425667da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for date, cluster in  zip(dates, W.argmax(axis=1)):\n",
    "    clusters[cluster].append(pd.to_datetime(date).strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f0371-e884-4b70-b821-51a921147abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,values in clusters.items():\n",
    "    print(f'cluster {key}:')\n",
    "    for v in values:\n",
    "        print(f'\\t{v}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13d6d1-29f0-4b39-9b0c-64aff5c7f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_terms_weights(H, leaks_vectorizer.get_feature_names())\n",
    "\n",
    "print_topics_udf(topics, total_topics=3, num_terms=15, display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f775ad-2f04-4ff3-ad5b-93984a4c6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ len(c) for c in clusters.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6c11f-cba0-4483-9f2e-2be614fbf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_latex_table(first_n, topics, clusters):\n",
    "\n",
    "    header = '\\multicolumn{2}{|c||}{topic 0} & \\multicolumn{2}{|c||}{topic 1} & \\multicolumn{2}{|c|}{topic 2} \\\\\\\\'\n",
    "\n",
    "    print('\\hline')\n",
    "    print(header)\n",
    "    print('\\hline')\n",
    "\n",
    "    for j in range(first_n):\n",
    "\n",
    "        for i in range(len(topics)):\n",
    "\n",
    "            if j < len(clusters[i]):\n",
    "                print(f'{clusters[i][j]} & ', end='')\n",
    "            else:\n",
    "                print('& ', end='')\n",
    "\n",
    "            word = topics[i][j,0].replace(\"_\", \"\\\\_\")\n",
    "            if i == len(topics) - 1:\n",
    "                print(f'{word} \\\\\\\\')\n",
    "            else:\n",
    "                print(f'{word} & ', end='')\n",
    "\n",
    "    print('\\hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c8d9-e7ef-4c06-8682-0da0aaf1940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_latex_table(15, topics, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-trail",
   "metadata": {},
   "source": [
    "# Communiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a21df-12e3-4ed6-a2af-5f74996ec54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "communiques = pd.read_csv('../data/communiques/cleaned.csv')\n",
    "\n",
    "# group by speaker\n",
    "grouped = communiques.drop(columns=['title']).groupby('date').apply(lambda s: ' '.join(s.story))\n",
    "# get speaker labels\n",
    "dates = grouped.index\n",
    "\n",
    "# preprocess\n",
    "comm_documents = [\n",
    "    ' '.join([token.lemma_.lower() for sentence in nlp(doc).sents for token in sentence\n",
    "              if filter_token(token)\n",
    "             ])\n",
    "             for doc in grouped.values\n",
    "]\n",
    "\n",
    "# leave out empty documents\n",
    "comm_documents = [d for d in comm_documents if len(d)>1]\n",
    "\n",
    "print(f'There are {len(comm_documents)} documents.')\n",
    "\n",
    "# apply collocations\n",
    "comm_documents = [\n",
    "    apply_bigram_colloc(apply_trigram_colloc(doc, trigram_colloc), bigram_colloc)\n",
    "    for doc in comm_documents]\n",
    "\n",
    "# need to filter out unidentified speakers first?\n",
    "\n",
    "# tfidf\n",
    "comm_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   min_df=2,\n",
    "                                   max_df=0.75,\n",
    "                                   smooth_idf=True,\n",
    "                                   sublinear_tf=False)\n",
    "\n",
    "comm_X = comm_vectorizer.fit_transform(comm_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102c9b5-78f3-48ce-ac4f-f6f482da67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b5af9-1c3b-44ac-9079-fbf67eb87d8b",
   "metadata": {},
   "source": [
    "### cross validation to find number of latent dimenstions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cca9f-89de-4bb6-be8c-3aad5fd46dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control with 6 latent dimensions\n",
    "# control_n_latent_dim = 3\n",
    "# M, N = 100, 150\n",
    "# noise = .8\n",
    "\n",
    "# U = np.random.rand(M, control_n_latent_dim)\n",
    "# Vt = np.random.rand(control_n_latent_dim, N)\n",
    "# control_X = np.dot(U, Vt) + noise*np.random.rand(M, N)\n",
    "\n",
    "# control_train_err, control_test_err = validate_nmf(control_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f059d9e-a81e-40aa-b4ae-94834af35416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate tfidf matrix\n",
    "#comm_train_err, comm_test_err = validate_nmf(comm_X.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c33b8d-e7d3-428a-a6f1-3abf24530233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# titles = [f'Control ({control_n_latent_dim} latent dimensions)', 'Communiques dates']\n",
    "# train_errs = [control_train_err, comm_train_err]\n",
    "# test_errs = [control_test_err, comm_test_err]\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "    \n",
    "#     ax.plot(*list(zip(*train_errs[i])), '-xb', label='train')\n",
    "#     ax.plot(*list(zip(*test_errs[i])), '-or', label='test')\n",
    "#     ax.set_title(titles[i])\n",
    "#     #ax.legend()\n",
    "        \n",
    "#     ax.set_xlabel('Number of factors')\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         ax.set_ylabel('Mean Squared Error')\n",
    "#         ax.axvline(control_n_latent_dim, color='k', dashes=[2,2])\n",
    "    \n",
    "# lines, labels = axes[0].get_legend_handles_labels()\n",
    "# fig.legend(lines, labels, loc='upper right')\n",
    "# fig.subplots_adjust(right=0.9)\n",
    "    \n",
    "# #fig.tight_layout()\n",
    "# fig.savefig('../figures/nmf-date-comm-validation.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95169ba1-80c8-44c5-a07f-27f26da68e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percent of entries in X which are 0: {np.round(np.mean(comm_X.A > 0)*100,2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-commercial",
   "metadata": {},
   "source": [
    "### 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=3,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          random_state=0,\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(comm_X.A)\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://stackoverflow.com/questions/22867620/putting-arrowheads-on-vectors-in-matplotlibs-3d-plot\n",
    "\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bb5cb-9620-4d87-b000-7051e1426fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(dates).strftime('%d/%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b196cc-2fb5-4147-8d84-2a1df9a90dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = [\n",
    "    (-.055,-.055,.0),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (-.055,-.055,.0),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (-.02,-.03,.045),\n",
    "    (-.05,-.055,-.05),\n",
    "    (.03,-.1,.0),\n",
    "    (.01,.01,.01),\n",
    "    (.01,.01,.01),\n",
    "    (-.055,-.055,.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# remove axis ticks\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "\n",
    "for i, label in enumerate(dates):\n",
    "\n",
    "    point = W[i,:]\n",
    "    \n",
    "    # plot\n",
    "    x,y,z = point\n",
    "    c = (min(max(x,0),1),min(max(y,0),1),min(max(z,0),1))\n",
    "    ax.scatter( x, y, z,\n",
    "                color=c,\n",
    "                edgecolor=None,\n",
    "                alpha=0.5,\n",
    "                s=100)\n",
    "    \n",
    "    # annotate\n",
    "    ax.text(x + offset[i][0],\n",
    "            y + offset[i][1],\n",
    "            z + offset[i][2],\n",
    "            pd.to_datetime(label).strftime('%d/%m'),\n",
    "            zdir=(1,1,0))\n",
    "    \n",
    "    # add arrow\n",
    "    if i>0:\n",
    "        previous_point = W[i-1,:]\n",
    "        px, py, pz = previous_point\n",
    "        #ax.arrow((px,x),(py,y),(pz,z),\n",
    "        #        color='black',\n",
    "        #        arrowstyle='-|>')\n",
    "        #ax.quiver(px,py,pz,\n",
    "        #          x,y,z,\n",
    "        #          color='black',\n",
    "        #          alpha=0.3,\n",
    "        #          lw=2)\n",
    "        arrow = Arrow3D([px,x],[py,y],[pz,z],\n",
    "                        mutation_scale=20,\n",
    "                        lw=1,\n",
    "                        arrowstyle='-|>',\n",
    "                        color='k',\n",
    "                        alpha=0.3)\n",
    "        ax.add_artist(arrow)\n",
    "        \n",
    "fig.savefig('../figures/3d-dates-comms.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172c401-4821-4523-a7a8-5ac9fde04f7c",
   "metadata": {},
   "source": [
    "### clustering\n",
    "Each axis captures the base topic of a particular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5a82f-13e0-4e46-996d-6f937db225e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(dates)} dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd824d52-c59e-4cf7-ae86-71f8b4a7bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3\n",
    "\n",
    "nmf = NMF(n_components=number_of_clusters,\n",
    "          init='nndsvd',\n",
    "          beta_loss='frobenius',\n",
    "          max_iter=1000,\n",
    "          alpha=0,\n",
    "          l1_ratio=0)\n",
    "\n",
    "W = nmf.fit_transform(comm_X.toarray())\n",
    "H = nmf.components_\n",
    "\n",
    "print(f'W shape: {W.shape}')\n",
    "print(f'H shape: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ed161-8dc9-4515-a006-7d5e99e91a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for date, cluster in  zip(dates, W.argmax(axis=1)):\n",
    "    clusters[cluster].append(pd.to_datetime(date).strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa0fba-5bbb-4619-a384-b305de9e0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,values in clusters.items():\n",
    "    print(f'cluster {key}:')\n",
    "    for v in values:\n",
    "        print(f'\\t{v}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201565c2-1d4f-4607-9a44-d07b04aeabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_terms_weights(H, comm_vectorizer.get_feature_names())\n",
    "\n",
    "print_topics_udf(topics, total_topics=3, num_terms=15, display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_latex_table(10, topics, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1f0f9-f8a5-472d-9255-74f3178b328c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
