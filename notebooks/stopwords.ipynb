{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-latex",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loaded-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "celtic-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/euroleaks/cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-mortality",
   "metadata": {},
   "source": [
    "## make stopwords out of speaker names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "usual-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/euroleaks/amend_names.json', 'r') as f:\n",
    "    amend_names = json.load(f)\n",
    "    \n",
    "# make stopwords out of names\n",
    "stopnames = []\n",
    "\n",
    "for names in amend_names.values():\n",
    "    for name in names:\n",
    "        if not re.search('\\[.*\\]', name):\n",
    "            stopnames += name.split(' ')\n",
    "            \n",
    "for name in df.speaker.unique():\n",
    "    if 'speaker' not in name:\n",
    "        stopnames += name.split(' ')\n",
    "\n",
    "stopnames = set(stopnames)\n",
    "        \n",
    "stopnames.remove('greek')\n",
    "stopnames.remove('representative')\n",
    "stopnames.remove('de')\n",
    "stopnames.remove('van')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supported-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopnames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "future-daisy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back of envelope calculation for how many stopnames there are\n",
    "np.sum(pd.Series(' '.join(df.speech).split()).apply(lambda s:\n",
    "    re.sub(r'[^\\w\\s]','',s) in stopnames\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-princess",
   "metadata": {},
   "source": [
    "## transription artifacts to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "different-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = [\n",
    "    'erm', # synonym for hmm in https://euroleaks.diem25.org/leaks/mar17ewg/\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swedish-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back of envelope calculation for how many artifacts there are\n",
    "np.sum(pd.Series(' '.join(df.speech).split()).apply(lambda s:\n",
    "    re.sub(r'[^\\w\\s]','',s) == 'erm'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-dakota",
   "metadata": {},
   "source": [
    "## weak speaker discriminants\n",
    "These are words that, when speakres are considered as documents, have high document frequency. In other words (hehe), these are words that all speakers use and therefore we can put them to little use for distinguishing between speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "herbal-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_stopwords = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tutorial-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.drop(columns=['timestamp','date']).groupby('speaker').apply(lambda s: ' '.join(s.speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "italian-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asajina/repos/master-thesis/.env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=list(stopnames)+artifacts+spacy_stopwords\n",
    ") # does tokenozation under the hood\n",
    "\n",
    "X = vectorizer.fit_transform(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chubby-manor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 4598)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faced-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent words by document frequency\n",
    "\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "freqs = np.mean(X.toarray() > 0, axis=0)\n",
    "\n",
    "sort_ix = np.argsort(freqs)[::-1] # descendingly\n",
    "words = words[sort_ix]\n",
    "freqs = freqs[sort_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "portable-webmaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uh',\n",
       " 'um',\n",
       " 'thank',\n",
       " 'greek',\n",
       " 'think',\n",
       " 'said',\n",
       " 'need',\n",
       " 'institutions',\n",
       " 'greece',\n",
       " 'point']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handled-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4598.000000\n",
       "mean        0.053030\n",
       "std         0.072518\n",
       "min         0.016949\n",
       "25%         0.016949\n",
       "50%         0.016949\n",
       "75%         0.050847\n",
       "90%         0.118644\n",
       "95%         0.203390\n",
       "99%         0.372881\n",
       "max         0.830508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(freqs).describe(percentiles=[.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "waiting-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(freqs > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "critical-director",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(freqs > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heard-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27770921032332"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back of envelope calculation for how many stopnames there are\n",
    "np.mean(pd.Series(' '.join(df.speech).split()).apply(lambda s:\n",
    "    re.sub(r'[^\\w\\s]','',s) in words[freqs > 0.1]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "corporate-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\n",
    "    'names': list(stopnames),\n",
    "    'artifacts': list(artifacts),\n",
    "    'weak_speaker_discriminants_0.5': list(words[freqs > 0.5]),\n",
    "    'weak_speaker_discriminants_0.1': list(words[freqs > 0.1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sharp-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to json\n",
    "import json\n",
    "\n",
    "json = json.dumps(stopwords)\n",
    "with open('../data/euroleaks/stopwords.json', 'w') as f:\n",
    "    f.write(json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
